{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeae92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "from autogen_core import (\n",
    "    FunctionCall,\n",
    "    MessageContext,\n",
    "    RoutedAgent,\n",
    "    message_handler,\n",
    ")\n",
    "from autogen_core.model_context import ChatCompletionContext\n",
    "from autogen_core.models import (\n",
    "    AssistantMessage,\n",
    "    ChatCompletionClient,\n",
    "    FunctionExecutionResult,\n",
    "    FunctionExecutionResultMessage,\n",
    "    LLMMessage,\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    ")\n",
    "from autogen_core.tools import ToolResult, Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f1adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    content: str\n",
    "\n",
    "\n",
    "class WorkbenchAgent(RoutedAgent):\n",
    "    def __init__(\n",
    "        self, model_client: ChatCompletionClient, model_context: ChatCompletionContext, workbench: Workbench\n",
    "    ) -> None:\n",
    "        super().__init__(\"An agent with a workbench\")\n",
    "        self._system_messages: List[LLMMessage] = [SystemMessage(content=\"You are a helpful AI assistant.\")]\n",
    "        self._model_client = model_client\n",
    "        self._model_context = model_context\n",
    "        self._workbench = workbench\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        # Add the user message to the model context.\n",
    "        await self._model_context.add_message(UserMessage(content=message.content, source=\"user\"))\n",
    "        print(\"---------User Message-----------\")\n",
    "        print(message.content)\n",
    "\n",
    "        # Run the chat completion with the tools.\n",
    "        create_result = await self._model_client.create(\n",
    "            messages=self._system_messages + (await self._model_context.get_messages()),\n",
    "            tools=(await self._workbench.list_tools()),\n",
    "            cancellation_token=ctx.cancellation_token,\n",
    "        )\n",
    "\n",
    "        # Run tool call loop.\n",
    "        while isinstance(create_result.content, list) and all(\n",
    "            isinstance(call, FunctionCall) for call in create_result.content\n",
    "        ):\n",
    "            print(\"---------Function Calls-----------\")\n",
    "            for call in create_result.content:\n",
    "                print(call)\n",
    "\n",
    "            # Add the function calls to the model context.\n",
    "            await self._model_context.add_message(AssistantMessage(content=create_result.content, source=\"assistant\"))\n",
    "\n",
    "            # Call the tools using the workbench.\n",
    "            print(\"---------Function Call Results-----------\")\n",
    "            results: List[ToolResult] = []\n",
    "            for call in create_result.content:\n",
    "                result = await self._workbench.call_tool(\n",
    "                    call.name, arguments=json.loads(call.arguments), cancellation_token=ctx.cancellation_token\n",
    "                )\n",
    "                results.append(result)\n",
    "                print(result)\n",
    "\n",
    "            # Add the function execution results to the model context.\n",
    "            await self._model_context.add_message(\n",
    "                FunctionExecutionResultMessage(\n",
    "                    content=[\n",
    "                        FunctionExecutionResult(\n",
    "                            call_id=call.id,\n",
    "                            content=result.to_text(),\n",
    "                            is_error=result.is_error,\n",
    "                            name=result.name,\n",
    "                        )\n",
    "                        for call, result in zip(create_result.content, results, strict=False)\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Run the chat completion again to reflect on the history and function execution results.\n",
    "            create_result = await self._model_client.create(\n",
    "                messages=self._system_messages + (await self._model_context.get_messages()),\n",
    "                tools=(await self._workbench.list_tools()),\n",
    "                cancellation_token=ctx.cancellation_token,\n",
    "            )\n",
    "\n",
    "        # Now we have a single message as the result.\n",
    "        assert isinstance(create_result.content, str)\n",
    "\n",
    "        print(\"---------Final Response-----------\")\n",
    "        print(create_result.content)\n",
    "\n",
    "        # Add the assistant message to the model context.\n",
    "        await self._model_context.add_message(AssistantMessage(content=create_result.content, source=\"assistant\"))\n",
    "\n",
    "        # Return the result as a message.\n",
    "        return Message(content=create_result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c38ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error constructing agent WebAgent/default\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 963, in _invoke_agent_factory\n",
      "    agent = cast(T, await agent)\n",
      "                    ^^^^^^^^^^^\n",
      "  File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 900, in factory_wrapper\n",
      "    maybe_agent_instance = agent_factory()\n",
      "                           ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_122409/1902546907.py\", line 21, in <lambda>\n",
      "    model_client=OpenAIChatCompletionClient(model=\"gpt-4.1-nano\"),\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 1447, in __init__\n",
      "    client = _openai_client_from_config(copied_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 134, in _openai_client_from_config\n",
      "    return AsyncOpenAI(**openai_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/openai/_client.py\", line 449, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3697, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/tmp/ipykernel_122409/1902546907.py\", line 12, in <module>\n",
      "  |     async with McpWorkbench(playwright_server_params) as workbench:  # type: ignore\n",
      "  |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/autogen_core/tools/_workbench.py\", line 191, in __aexit__\n",
      "  |     await self.stop()\n",
      "  |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/autogen_ext/tools/mcp/_workbench.py\", line 349, in stop\n",
      "  |     await self._actor.close()\n",
      "  |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/autogen_ext/tools/mcp/_actor.py\", line 78, in close\n",
      "  |     await self._shutdown_future\n",
      "  |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/autogen_ext/tools/mcp/_actor.py\", line 85, in _run_actor\n",
      "  |     async with create_mcp_server_session(self.server_params) as session:\n",
      "  |   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/autogen_ext/tools/mcp/_session.py\", line 27, in create_mcp_server_session\n",
      "  |     async with sse_client(**server_params.model_dump(exclude={\"type\"})) as (read, write):\n",
      "  |   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/mcp/client/sse.py\", line 54, in sse_client\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    |     yield\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n",
      "    |     resp = await self._pool.handle_async_request(req)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n",
      "    |     raise exc from None\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n",
      "    |     response = await connection.handle_async_request(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n",
      "    |     raise exc\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n",
      "    |     stream = await self._connect(request)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n",
      "    |     stream = await self._network_backend.connect_tcp(**kwargs)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n",
      "    |     return await self._backend.connect_tcp(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n",
      "    |     with map_exceptions(exc_map):\n",
      "    |   File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    |     raise to_exc(exc) from exc\n",
      "    | httpcore.ConnectError: All connection attempts failed\n",
      "    | \n",
      "    | The above exception was the direct cause of the following exception:\n",
      "    | \n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/mcp/client/sse.py\", line 60, in sse_client\n",
      "    |     async with aconnect_sse(\n",
      "    |   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpx_sse/_api.py\", line 74, in aconnect_sse\n",
      "    |     async with client.stream(method, url, headers=headers, **kwargs) as response:\n",
      "    |   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1583, in stream\n",
      "    |     response = await self.send(\n",
      "    |                ^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n",
      "    |     response = await self._send_handling_auth(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n",
      "    |     response = await self._send_handling_redirects(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n",
      "    |     response = await self._send_single_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n",
      "    |     response = await transport.handle_async_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n",
      "    |     with map_httpcore_exceptions():\n",
      "    |   File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |   File \"/home/administrator/Desktop/datascience/github/autogen-examples/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    |     raise mapped_exc(message) from exc\n",
      "    | httpx.ConnectError: All connection attempts failed\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from autogen_core import AgentId, SingleThreadedAgentRuntime\n",
    "from autogen_core.model_context import BufferedChatCompletionContext\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.tools.mcp import McpWorkbench, SseServerParams\n",
    "\n",
    "playwright_server_params = SseServerParams(\n",
    "    url=\"http://localhost:8931/sse\",\n",
    ")\n",
    "\n",
    "# Start the workbench in a context manager.\n",
    "# You can also start and stop the workbench using `workbench.start()` and `workbench.stop()`.\n",
    "async with McpWorkbench(playwright_server_params) as workbench:  # type: ignore\n",
    "    # Create a single-threaded agent runtime.\n",
    "    runtime = SingleThreadedAgentRuntime()\n",
    "\n",
    "    # Register the agent with the runtime.\n",
    "    await WorkbenchAgent.register(\n",
    "        runtime=runtime,\n",
    "        type=\"WebAgent\",\n",
    "        factory=lambda: WorkbenchAgent(\n",
    "            model_client=OpenAIChatCompletionClient(model=\"gpt-4.1-nano\"),\n",
    "            model_context=BufferedChatCompletionContext(buffer_size=10),\n",
    "            workbench=workbench,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Start the runtime.\n",
    "    runtime.start()\n",
    "\n",
    "    # Send a message to the agent.\n",
    "    await runtime.send_message(\n",
    "        Message(content=\"Use Bing to find out the address of Microsoft Building 99\"),\n",
    "        recipient=AgentId(\"WebAgent\", \"default\"),\n",
    "    )\n",
    "\n",
    "    # Stop the runtime.\n",
    "    await runtime.stop()\n",
    "from autogen_core import AgentId, SingleThreadedAgentRuntime\n",
    "from autogen_core.model_context import BufferedChatCompletionContext\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.tools.mcp import McpWorkbench, SseServerParams\n",
    "\n",
    "playwright_server_params = SseServerParams(\n",
    "    url=\"http://localhost:8931/sse\",\n",
    ")\n",
    "\n",
    "# Start the workbench in a context manager.\n",
    "# You can also start and stop the workbench using `workbench.start()` and `workbench.stop()`.\n",
    "async with McpWorkbench(playwright_server_params) as workbench:  # type: ignore\n",
    "    # Create a single-threaded agent runtime.\n",
    "    runtime = SingleThreadedAgentRuntime()\n",
    "\n",
    "    # Register the agent with the runtime.\n",
    "    await WorkbenchAgent.register(\n",
    "        runtime=runtime,\n",
    "        type=\"WebAgent\",\n",
    "        factory=lambda: WorkbenchAgent(\n",
    "            model_client=OpenAIChatCompletionClient(model=\"gpt-4.1-nano\"),\n",
    "            model_context=BufferedChatCompletionContext(buffer_size=10),\n",
    "            workbench=workbench,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Start the runtime.\n",
    "    runtime.start()\n",
    "\n",
    "    # Send a message to the agent.\n",
    "    await runtime.send_message(\n",
    "        Message(content=\"Use Bing to find out the address of Microsoft Building 99\"),\n",
    "        recipient=AgentId(\"WebAgent\", \"default\"),\n",
    "    )\n",
    "\n",
    "    # Stop the runtime.\n",
    "    await runtime.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fb612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46ddb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
