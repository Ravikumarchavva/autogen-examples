{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfaad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core import CancellationToken, TypeSubscription, TopicId, AgentId, type_subscription\n",
    "from autogen_agentchat.messages import TextMessage, Image, UserMessage\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import DiGraph, DiGraphBuilder, GraphFlow\n",
    "from autogen_core.code_executor import CodeBlock, CodeExecutor\n",
    "from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n",
    "from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\n",
    "from autogen_ext.tools.code_execution import PythonCodeExecutionTool\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "working_dir = Path(\"coding\")\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n",
    "\n",
    "inst_sub = TypeSubscription(topic_type=\"instruction\", agent_type=\"assistant_agent\")\n",
    "exec_sub = TypeSubscription(topic_type=\"execution\", agent_type=\"python_agent\")\n",
    "res_sub = TypeSubscription(topic_type=\"result\", agent_type=\"assistant_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d812ecfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ABCMeta.register() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m runtime.add_subscription(res_sub)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Define Agents\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m assistant = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mAssistantAgent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mruntime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43massistant_agent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAssistantAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are the assisting LLM.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Python execution agent (simple RoutedAgent)\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Uses LocalCommandLineCodeExecutor to execute code blocks\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautogen_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RoutedAgent, message_handler, MessageContext\n",
      "\u001b[31mTypeError\u001b[39m: ABCMeta.register() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "from autogen_core import SingleThreadedAgentRuntime\n",
    "from autogen_core.models import AssistantMessage\n",
    "runtime = SingleThreadedAgentRuntime()\n",
    "\n",
    "# Register subscriptions\n",
    "await runtime.add_subscription(inst_sub)\n",
    "await runtime.add_subscription(exec_sub)\n",
    "await runtime.add_subscription(res_sub)\n",
    "\n",
    "# Define Agents\n",
    "assistant = await AssistantAgent.register(\n",
    "    runtime,\n",
    "    \"assistant_agent\",\n",
    "    lambda: AssistantAgent(model_client=model_client, system_message=\"You are the assisting LLM.\")\n",
    ")\n",
    "\n",
    "# Python execution agent (simple RoutedAgent)\n",
    "# Uses LocalCommandLineCodeExecutor to execute code blocks\n",
    "from autogen_core import RoutedAgent, message_handler, MessageContext\n",
    "from autogen_core.models import LLMMessage\n",
    "\n",
    "class PythonAgent(RoutedAgent):\n",
    "    @message_handler\n",
    "    async def run_code(self, message: AssistantMessage, ctx: MessageContext):\n",
    "        code = message.content\n",
    "        with LocalCommandLineCodeExecutor(work_dir=working_dir) as executor:\n",
    "            result = await executor.execute_code_blocks(\n",
    "                code_blocks=[CodeBlock(language=\"python\", code=code)],\n",
    "                cancellation_token=CancellationToken(),\n",
    "            )\n",
    "        output = result.output or \"\"\n",
    "        await self.publish_message(\n",
    "            AssistantMessage(content=output),\n",
    "            topic_id=TopicId(type=\"result\", source=ctx.topic_id.source)\n",
    "        )\n",
    "\n",
    "python_agent = await PythonAgent.register(\n",
    "    runtime, \"python_agent\", lambda: PythonAgent(\"Python Executor\")\n",
    ")\n",
    "\n",
    "# Start runtime\n",
    "runtime.start()\n",
    "\n",
    "# Simulate user question\n",
    "user_topic = \"session1\"\n",
    "await runtime.publish_message(\n",
    "    UserMessage(content=\"Make a plot of x vs y\"),\n",
    "    topic_id=TopicId(type=\"instruction\", source=user_topic),\n",
    ")\n",
    "\n",
    "await runtime.stop_when_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010fcff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_agent = AssistantAgent(\n",
    "    name=\"assistant_agent\",\n",
    "    description=\"An assistant agent that analyses user queries and provides helpful responses.\",\n",
    "    system_message=\"\"\"\n",
    "    You are a helpful assistant you analyze user queries and provide helpful responses.\n",
    "\n",
    "    if the user asks a question which requires code execution, you should delegate the task to the code agent by saying clear instructions about what code to run and what the expected output is.\n",
    "    \"\"\",\n",
    "    model_client=model_client\n",
    ")\n",
    "code_agent = AssistantAgent(\n",
    "    name=\"code_agent\",\n",
    "    description=\"A code generation agent that generates code snippets and returns the output.\",\n",
    "    system_message=\"\"\"\n",
    "    You are a code generation agent you generate code snippets and use the code execution tool to run them.\n",
    "    \"\"\",\n",
    "    model_client=model_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7435d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DiGraphBuilder()\n",
    "builder.add_node(assistant_agent).add_node(code_agent)\n",
    "builder.add_edge(assistant_agent, code_agent)\n",
    "\n",
    "graph = builder.build()\n",
    "\n",
    "flow = GraphFlow(participants=[assistant_agent, code_agent], graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72c4a77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An assistant agent that analyses user queries and provides helpful responses.',\n",
       " 'A code generation agent that generates code snippets in python and returns the output.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow._participant_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a7b4ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='75029450-5439-401c-8099-8c6c0d90cc06' source='user' models_usage=None metadata={} created_at=datetime.datetime(2025, 8, 10, 18, 45, 12, 134068, tzinfo=datetime.timezone.utc) content=\"Give me a code snippet that prints 'Hello, World!'\" type='TextMessage'\n",
      "id='a6f8320a-2288-4319-a1fc-b737f06266d0' source='assistant_agent' models_usage=RequestUsage(prompt_tokens=78, completion_tokens=52) metadata={} created_at=datetime.datetime(2025, 8, 10, 18, 45, 13, 967532, tzinfo=datetime.timezone.utc) content=\"Sure! Here's a simple code snippet in Python that prints 'Hello, World!':\\n\\n```python\\nprint('Hello, World!')\\n```\\n\\nYou can run this code in any Python environment, and it will display `Hello, World!` as the output.\" type='TextMessage'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for code_agent_1dd06c88-c353-4c1a-a4c6-42079b6daa48/1dd06c88-c353-4c1a-a4c6-42079b6daa48\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 989, in on_messages_stream\n",
      "    async for output_event in self._process_model_result(\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1299, in _process_model_result\n",
      "    async for reflection_response in cls._reflect_on_tool_use_flow(\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1446, in _reflect_on_tool_use_flow\n",
      "    reflection_result = await model_client.create(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 660, in create\n",
      "    create_params = self._process_create_args(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 602, in _process_create_args\n",
      "    oai_messages_nested = [\n",
      "                          ^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 603, in <listcomp>\n",
      "    to_oai_type(\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 184, in to_oai_type\n",
      "    result = transformer(message, context)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_transformation\\registry.py\", line 64, in transformer\n",
      "    kwargs.update(func(message, context))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_message_transform.py\", line 250, in _set_tool_calls\n",
      "    \"tool_calls\": [func_call_to_oai(x) for x in message.content],\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_message_transform.py\", line 250, in <listcomp>\n",
      "    \"tool_calls\": [func_call_to_oai(x) for x in message.content],\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_message_transform.py\", line 176, in func_call_to_oai\n",
      "    return ChatCompletionMessageToolCallParam(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\chavv\\AppData\\Roaming\\uv\\python\\cpython-3.11.12-windows-x86_64-none\\Lib\\typing.py\", line 1289, in __call__\n",
      "    result = self.__origin__(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\chavv\\AppData\\Roaming\\uv\\python\\cpython-3.11.12-windows-x86_64-none\\Lib\\typing.py\", line 486, in __call__\n",
      "    raise TypeError(f\"Cannot instantiate {self!r}\")\n",
      "TypeError: Cannot instantiate typing.Union\n",
      "Error processing publish message for assistant_agent_1dd06c88-c353-4c1a-a4c6-42079b6daa48/1dd06c88-c353-4c1a-a4c6-42079b6daa48\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 195, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for GraphManager_1dd06c88-c353-4c1a-a4c6-42079b6daa48/1dd06c88-c353-4c1a-a4c6-42079b6daa48\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat_manager.py\", line 270, in handle_group_chat_error\n",
      "    await self._signal_termination_with_error(message.error)\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat_manager.py\", line 255, in _signal_termination_with_error\n",
      "    await self.publish_message(\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 151, in publish_message\n",
      "    await self._runtime.publish_message(message, topic_id, sender=self.id, cancellation_token=cancellation_token)\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 420, in publish_message\n",
      "    await self._message_queue.put(\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_queue.py\", line 139, in put\n",
      "    return self.put_nowait(item)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_queue.py\", line 149, in put_nowait\n",
      "    raise QueueShutDown\n",
      "autogen_core._queue.QueueShutDown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='f8d2ae6f-d002-4641-82c0-d9d2c3cdee6d' source='code_agent' models_usage=RequestUsage(prompt_tokens=153, completion_tokens=20) metadata={} created_at=datetime.datetime(2025, 8, 10, 18, 45, 15, 434743, tzinfo=datetime.timezone.utc) content=[FunctionCall(id='call_InVgLelK8tnERbD4tDnyQagH', arguments='{\"code\":\"print(\\'Hello, World!\\')\"}', name='CodeExecutor')] type='ToolCallRequestEvent'\n",
      "id='720b1834-2b4f-4bd5-b571-45094b93059a' source='code_agent' models_usage=None metadata={} created_at=datetime.datetime(2025, 8, 10, 18, 45, 15, 435742, tzinfo=datetime.timezone.utc) content=[FunctionExecutionResult(content=\"'coroutine' object has no attribute 'execute_code_blocks'\", name='CodeExecutor', call_id='call_InVgLelK8tnERbD4tDnyQagH', is_error=True)] type='ToolCallExecutionEvent'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "TypeError: Cannot instantiate typing.Union\nTraceback:\nTraceback (most recent call last):\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py\", line 502, in stop_runtime\n    await self._runtime.stop_when_idle()\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 851, in stop_when_idle\n    await self._run_context.stop_when_idle()\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 121, in stop_when_idle\n    await self._run_task\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 110, in _run\n    await self._runtime._process_next()  # type: ignore\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 678, in _process_next\n    raise e\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 624, in _process_publish\n    await asyncio.gather(*responses)\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 619, in _on_message\n    raise e\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n    return await h(self, message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n    return_value = await func(self, message, ctx)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 989, in on_messages_stream\n    async for output_event in self._process_model_result(\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1299, in _process_model_result\n    async for reflection_response in cls._reflect_on_tool_use_flow(\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1446, in _reflect_on_tool_use_flow\n    reflection_result = await model_client.create(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 660, in create\n    create_params = self._process_create_args(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 602, in _process_create_args\n    oai_messages_nested = [\n                          ^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 603, in <listcomp>\n    to_oai_type(\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 184, in to_oai_type\n    result = transformer(message, context)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_transformation\\registry.py\", line 64, in transformer\n    kwargs.update(func(message, context))\n                  ^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_message_transform.py\", line 250, in _set_tool_calls\n    \"tool_calls\": [func_call_to_oai(x) for x in message.content],\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_message_transform.py\", line 250, in <listcomp>\n    \"tool_calls\": [func_call_to_oai(x) for x in message.content],\n                   ^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_message_transform.py\", line 176, in func_call_to_oai\n    return ChatCompletionMessageToolCallParam(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\chavv\\AppData\\Roaming\\uv\\python\\cpython-3.11.12-windows-x86_64-none\\Lib\\typing.py\", line 1289, in __call__\n    result = self.__origin__(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\chavv\\AppData\\Roaming\\uv\\python\\cpython-3.11.12-windows-x86_64-none\\Lib\\typing.py\", line 486, in __call__\n    raise TypeError(f\"Cannot instantiate {self!r}\")\n\nTypeError: Cannot instantiate typing.Union\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use `asyncio.run(...)` and wrap the below in a async function when running in a script.\u001b[39;00m\n\u001b[32m      2\u001b[39m stream = flow.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33mGive me a code snippet that prints \u001b[39m\u001b[33m'\u001b[39m\u001b[33mHello, World!\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m stream:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(event)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Use Console(flow.run_stream(...)) for better formatting in console.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:554\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    555\u001b[39m     stop_reason = message.message.content\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: TypeError: Cannot instantiate typing.Union\nTraceback:\nTraceback (most recent call last):\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py\", line 502, in stop_runtime\n    await self._runtime.stop_when_idle()\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 851, in stop_when_idle\n    await self._run_context.stop_when_idle()\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 121, in stop_when_idle\n    await self._run_task\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 110, in _run\n    await self._runtime._process_next()  # type: ignore\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 678, in _process_next\n    raise e\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 624, in _process_publish\n    await asyncio.gather(*responses)\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 619, in _on_message\n    raise e\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n    return await h(self, message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n    return_value = await func(self, message, ctx)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 989, in on_messages_stream\n    async for output_event in self._process_model_result(\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1299, in _process_model_result\n    async for reflection_response in cls._reflect_on_tool_use_flow(\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1446, in _reflect_on_tool_use_flow\n    reflection_result = await model_client.create(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 660, in create\n    create_params = self._process_create_args(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 602, in _process_create_args\n    oai_messages_nested = [\n                          ^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 603, in <listcomp>\n    to_oai_type(\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 184, in to_oai_type\n    result = transformer(message, context)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_transformation\\registry.py\", line 64, in transformer\n    kwargs.update(func(message, context))\n                  ^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_message_transform.py\", line 250, in _set_tool_calls\n    \"tool_calls\": [func_call_to_oai(x) for x in message.content],\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_message_transform.py\", line 250, in <listcomp>\n    \"tool_calls\": [func_call_to_oai(x) for x in message.content],\n                   ^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_message_transform.py\", line 176, in func_call_to_oai\n    return ChatCompletionMessageToolCallParam(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\chavv\\AppData\\Roaming\\uv\\python\\cpython-3.11.12-windows-x86_64-none\\Lib\\typing.py\", line 1289, in __call__\n    result = self.__origin__(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\chavv\\AppData\\Roaming\\uv\\python\\cpython-3.11.12-windows-x86_64-none\\Lib\\typing.py\", line 486, in __call__\n    raise TypeError(f\"Cannot instantiate {self!r}\")\n\nTypeError: Cannot instantiate typing.Union\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\autogen-examples\\.venv\\Lib\\site-packages\\pygments\\regexopt.py:77: RuntimeWarning: coroutine 'DockerCommandLineCodeExecutor.start' was never awaited\n",
      "  '|'.join(regex_opt_inner(list(group[1]), '')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# Use `asyncio.run(...)` and wrap the below in a async function when running in a script.\n",
    "stream = flow.run_stream(task=\"Give me a code snippet that prints 'Hello, World!'\")\n",
    "async for event in stream:  # type: ignore\n",
    "    print(event)\n",
    "# Use Console(flow.run_stream(...)) for better formatting in console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1946313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
